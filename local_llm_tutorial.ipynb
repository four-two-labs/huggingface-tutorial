{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af83c47",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<style>\n",
    "/* Custom styling for Jupyter notebook */\n",
    ":root {\n",
    "    --main-bg-color: #f8f9fa;\n",
    "    --text-color: #212529;\n",
    "    --link-color: #3498db;\n",
    "    --code-bg: #f8f9fa;\n",
    "    --success-color: #2ecc71;\n",
    "    --error-color: #e74c3c;\n",
    "    --border-color: #dee2e6;\n",
    "    --heading-color: #2c3e50;\n",
    "    --notebook-width: 800px;  /* Control notebook width here */\n",
    "}\n",
    "\n",
    "body {\n",
    "    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n",
    "    line-height: 1.6;\n",
    "    max-width: var(--notebook-width);\n",
    "    margin: 0 auto;\n",
    "    padding: 2rem;\n",
    "    background-color: var(--main-bg-color);\n",
    "    color: var(--text-color);\n",
    "}\n",
    "\n",
    "/* Force elements to respect max width */\n",
    ".jp-OutputArea-output {\n",
    "    max-width: var(--notebook-width);\n",
    "    margin: 0 auto;\n",
    "}\n",
    "\n",
    "/* Table responsiveness */\n",
    "div.jp-RenderedHTMLCommon table {\n",
    "    max-width: var(--notebook-width); \n",
    "    overflow-x: auto;\n",
    "    display: block;\n",
    "}\n",
    "\n",
    "/* Code cells width control */\n",
    ".jp-Cell-inputWrapper, .jp-Cell-outputWrapper {\n",
    "    max-width: var(--notebook-width);\n",
    "}\n",
    "\n",
    "/* Image width control */\n",
    "img {\n",
    "    max-width: 100%;\n",
    "    margin: 0 auto;\n",
    "    display: block;\n",
    "}\n",
    "\n",
    "/* Rest of the styles remain the same */\n",
    ".notebook {\n",
    "    background: white;\n",
    "    padding: 2rem;\n",
    "    border-radius: 8px;\n",
    "    box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
    "}\n",
    "\n",
    "div.cell {\n",
    "    margin-bottom: 1.5rem;\n",
    "    border: none;\n",
    "}\n",
    "\n",
    "div.text_cell_render {\n",
    "    padding: 1rem 0;\n",
    "    line-height: 1.8;\n",
    "}\n",
    "\n",
    "h1, h2, h3, h4 {\n",
    "    color: var(--heading-color);\n",
    "    margin-top: 2rem;\n",
    "    margin-bottom: 1rem;\n",
    "    font-weight: 600;\n",
    "}\n",
    "\n",
    "h1 { font-size: 2.5rem; }\n",
    "h2 { font-size: 2rem; }\n",
    "h3 { font-size: 1.5rem; }\n",
    "\n",
    "div.input_area {\n",
    "    background-color: var(--code-bg);\n",
    "    border-radius: 6px;\n",
    "    padding: 1rem;\n",
    "    margin: 0.5rem 0;\n",
    "}\n",
    "\n",
    "div.output_area {\n",
    "    margin: 0.5rem 0;\n",
    "    padding: 1rem;\n",
    "    background: white;\n",
    "    border-radius: 6px;\n",
    "}\n",
    "\n",
    "div.output_text pre {\n",
    "    border-left: 4px solid var(--success-color);\n",
    "    margin: 0.5rem 0;\n",
    "    padding-left: 1rem;\n",
    "}\n",
    "\n",
    "div.output_stderr pre {\n",
    "    background-color: #fff5f5;\n",
    "    border-left: 4px solid var(--error-color);\n",
    "    color: var(--error-color);\n",
    "    padding-left: 1rem;\n",
    "}\n",
    "\n",
    "code {\n",
    "    font-family: 'Fira Code', 'JetBrains Mono', monospace;\n",
    "    font-size: 0.9em;\n",
    "}\n",
    "\n",
    "pre {\n",
    "    background-color: var(--code-bg);\n",
    "    padding: 1rem;\n",
    "    border-radius: 6px;\n",
    "    overflow-x: auto;\n",
    "}\n",
    "\n",
    "a {\n",
    "    color: var(--link-color);\n",
    "    text-decoration: none;\n",
    "}\n",
    "\n",
    "a:hover {\n",
    "    text-decoration: underline;\n",
    "}\n",
    "\n",
    "img {\n",
    "    max-width: 100%;\n",
    "    height: auto;\n",
    "    border-radius: 4px;\n",
    "    margin: 1rem 0;\n",
    "}\n",
    "\n",
    "table {\n",
    "    width: 100%;\n",
    "    margin: 1rem 0;\n",
    "    border-collapse: collapse;\n",
    "}\n",
    "\n",
    "th, td {\n",
    "    padding: 0.75rem;\n",
    "    border: 1px solid var(--border-color);\n",
    "}\n",
    "\n",
    "th {\n",
    "    background-color: var(--code-bg);\n",
    "    font-weight: 600;\n",
    "}\n",
    "\n",
    "/* Syntax highlighting */\n",
    ".highlight .k { color: #8959a8; } /* Keyword */\n",
    ".highlight .n { color: #4d4d4c; } /* Name */\n",
    ".highlight .s { color: #718c00; } /* String */\n",
    ".highlight .c { color: #8e908c; font-style: italic; } /* Comment */\n",
    "\n",
    "/* Dark mode support */\n",
    "@media (prefers-color-scheme: dark) {\n",
    "    :root {\n",
    "        --main-bg-color: #1a1a1a;\n",
    "        --text-color: #e0e0e0;\n",
    "        --link-color: #61afef;\n",
    "        --code-bg: #282c34;\n",
    "        --success-color: #98c379;\n",
    "        --error-color: #e06c75;\n",
    "        --border-color: #3e4451;\n",
    "        --heading-color: #61afef;\n",
    "    }\n",
    "\n",
    "    body {\n",
    "        background-color: var(--main-bg-color);\n",
    "        color: var(--text-color);\n",
    "    }\n",
    "\n",
    "    .notebook {\n",
    "        background: #282c34;\n",
    "    }\n",
    "\n",
    "    div.output_area {\n",
    "        background: #282c34;\n",
    "    }\n",
    "\n",
    "    .highlight .n { color: #abb2bf; }\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd76c2",
   "metadata": {},
   "source": [
    "# Local LLM Tutorial: Streaming Text Generation\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Understanding the Pipeline](#understanding-the-pipeline)\n",
    "3. [Model Download](#model-download)\n",
    "4. [Model Loading](#model-loading)\n",
    "5. [Setting Up Streaming](#streaming)\n",
    "6. [Generation Parameters](#parameters)\n",
    "7. [KV Caching Optimization](#kv-caching)\n",
    "8. [Running the Generation](#running)\n",
    "9. [Try Different Queries](#queries)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Language models have become increasingly powerful, but their practical deployment often requires\n",
    "efficient handling of text generation, especially in interactive applications. Streaming generation\n",
    "is crucial for creating responsive user experiences, as it allows for real-time display of\n",
    "generated text rather than waiting for the entire output.\n",
    "\n",
    "### What We'll Cover:\n",
    "\n",
    "1. Set up a local language model using the Hugging Face pipeline\n",
    "2. Implement streaming text generation\n",
    "3. Configure generation parameters for optimal output\n",
    "4. Understand the underlying components and their interactions\n",
    "\n",
    "> **Important Note**: Some models (such as Mistral models) on the Hugging Face Hub require accepting terms of use (gated models).\n",
    "> \n",
    "> Requirements:\n",
    "> - Log in to your Hugging Face account (or create one)\n",
    "> - Accept the terms on the model's page\n",
    "> - Set your Hugging Face API token as the `HF_TOKEN` variable in a `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09603236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "import torch\n",
    "import dotenv\n",
    "from transformers import pipeline\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "dotenv.load_dotenv('.env')\n",
    "\n",
    "# Choose a Model\n",
    "model_id = 'deepcogito/cogito-v1-preview-llama-3B'\n",
    "#model_id = 'HuggingFaceTB/SmolLM2-1.7B-Instruct'\n",
    "#model_id = 'mistralai/Mistral-7B-Instruct-v0.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002b886",
   "metadata": {},
   "source": [
    "## Understanding the Pipeline\n",
    "\n",
    "The Hugging Face pipeline is a high-level abstraction that combines several components:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Input[\"Input Text\"] --> Tokenizer\n",
    "    Tokenizer --> Preprocessor\n",
    "    Preprocessor --> Model\n",
    "    Model --> Postprocessor\n",
    "    Postprocessor --> Output[\"Output Text\"]\n",
    "    Output -.->|Forward Pass| Model\n",
    "    \n",
    "    style Input fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    style Tokenizer fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    style Preprocessor fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    style Model fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    style Postprocessor fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    style Output fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    \n",
    "    linkStyle 5 stroke:#333,stroke-width:1px,stroke-dasharray:5 5\n",
    "```\n",
    "\n",
    "### Core Components\n",
    "\n",
    "#### Model\n",
    "- Core language model (causal LM)\n",
    "- Transformer architecture\n",
    "- Model weights and definition\n",
    "- Forward pass handling\n",
    "\n",
    "#### Tokenizer\n",
    "- Text to token conversion\n",
    "- Subword tokenization\n",
    "- Special token handling\n",
    "- Vocabulary management\n",
    "\n",
    "#### Preprocessor\n",
    "- Input preparation\n",
    "- Chat template application\n",
    "- Padding and truncation\n",
    "- Attention mask creation\n",
    "\n",
    "#### Postprocessor\n",
    "- Output processing\n",
    "- Token ID decoding\n",
    "- Special token removal\n",
    "- Output formatting\n",
    "\n",
    "---\n",
    "\n",
    "## Model Download\n",
    "\n",
    "Available models for this tutorial:\n",
    "\n",
    "| Model             | Size | Description               |\n",
    "|:-----------------|:-----:|:--------------------------|\n",
    "| cogito-v1-preview | 3B   | Fast, lightweight model  |\n",
    "| SmolLM2          | 1.7B | Efficient instruction    |\n",
    "| Mistral          | 7B   | Powerful, state-of-the-art|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with optimized settings\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_id,\n",
    "    model_kwargs={'torch_dtype': torch.bfloat16},\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a17d0",
   "metadata": {},
   "source": [
    "## Setting Up Streaming\n",
    "\n",
    "The streaming process involves three key components working together:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Main[\"Main Thread\"] -->|Sends Input| Queue[\"Token Queue\"]\n",
    "    Worker[\"Worker Thread\"] -->|Generates| Queue\n",
    "    Queue -->|Buffers| Output[\"Output Text\"]\n",
    "    \n",
    "    style Main fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    style Worker fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    style Queue fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    style Output fill:#f0f0ff,stroke:#9999ff,stroke-width:2px\n",
    "    \n",
    "    linkStyle 0 stroke:#333,stroke-width:1px\n",
    "    linkStyle 1 stroke:#333,stroke-width:1px\n",
    "    linkStyle 2 stroke:#333,stroke-width:1px\n",
    "```\n",
    "\n",
    "During streaming generation:\n",
    "- The main thread sends inputs to the model and monitors the token queue\n",
    "- The worker thread continuously generates tokens and adds them to the queue\n",
    "- Tokens are consumed from the queue and displayed in real-time\n",
    "\n",
    "---\n",
    "\n",
    "## Generation Parameters\n",
    "\n",
    "### Precision Controls\n",
    "| Parameter        | Value | Purpose          |\n",
    "|:----------------|:-----:|:-----------------|\n",
    "| `max_new_tokens`| 2048  | Output length    |\n",
    "| `do_sample`     | True  | Enable sampling  |\n",
    "\n",
    "### Creative Controls\n",
    "| Parameter     | Value | Purpose          |\n",
    "|:-------------|:-----:|:-----------------|\n",
    "| `temperature`| 0.7   | Randomness       |\n",
    "| `top_p`      | 0.95  | Diversity        |\n",
    "\n",
    "### KV Cache Controls\n",
    "| Parameter             | Value    | Purpose                      |\n",
    "|:---------------------|:--------:|:-----------------------------|\n",
    "| `use_cache`          | True     | Enable KV caching            |\n",
    "| `cache_implementation`| dynamic  | Caching strategy             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf90eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the streamer\n",
    "streamer = TextIteratorStreamer(\n",
    "    pipe.tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0e006",
   "metadata": {},
   "source": [
    "## KV Caching Optimization\n",
    "\n",
    "KV (Key-Value) caching is a crucial optimization technique for efficient text generation with transformer models.\n",
    "\n",
    "### What is KV Caching?\n",
    "\n",
    "During autoregressive generation, a model needs to process all previously generated tokens to produce the next token. KV caching stores the intermediate attention key (K) and value (V) states from previous generation steps, avoiding redundant recalculations.\n",
    "\n",
    "```\n",
    "Without KV Cache:\n",
    "[token₁] → compute KV → output token₁\n",
    "[token₁, token₂] → compute KV → output token₂\n",
    "[token₁, token₂, token₃] → compute KV → output token₃\n",
    "...\n",
    "\n",
    "With KV Cache:\n",
    "[token₁] → compute KV → cache KV → output token₁\n",
    "[token₂] → use cached KV + compute new KV → cache KV → output token₂\n",
    "[token₃] → use cached KV + compute new KV → cache KV → output token₃\n",
    "...\n",
    "```\n",
    "\n",
    "### Caching Implementations\n",
    "\n",
    "#### Static Caching\n",
    "- Pre-allocates memory for the entire sequence length\n",
    "- More memory usage but consistent performance\n",
    "- Best for fixed-length generations\n",
    "\n",
    "#### Dynamic Caching\n",
    "- Allocates memory as needed during generation\n",
    "- More efficient for variable-length outputs\n",
    "- Reduces initial memory footprint\n",
    "\n",
    "#### Offloaded Caching\n",
    "- Stores KV cache in CPU memory instead of GPU\n",
    "- Enables generation with larger context lengths\n",
    "- Trades speed for memory efficiency\n",
    "\n",
    "### Performance Benefits\n",
    "\n",
    "| Aspect        | Improvement     | Impact                              |\n",
    "|:--------------|:----------------:|:------------------------------------|\n",
    "| Speed         | 2-4× faster     | Reduced time for token generation   |\n",
    "| Computation   | O(n) → O(1)*    | Constant time for each new token    |\n",
    "| Memory        | Higher usage    | Requires more GPU VRAM              |\n",
    "| Scalability   | Limited by VRAM | Max context depends on memory       |\n",
    "\n",
    "*For attention computation per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4abd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the generation parameters\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
    "    {'role': 'user', 'content': 'What are black holes?'}\n",
    "]\n",
    "\n",
    "generation_kwargs = {\n",
    "    # Input configuration\n",
    "    'text_inputs': messages,\n",
    "    'streamer': streamer,\n",
    "\n",
    "    # KV Cache settings\n",
    "    'use_cache': True,\n",
    "    'cache_implementation': 'dynamic',  # 'dynamic', 'static' or offloaded\n",
    "    \n",
    "    # Generation settings\n",
    "    'max_new_tokens': 2048,\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.95\n",
    "}\n",
    "\n",
    "# Start generation in a thread\n",
    "thread = Thread(target=pipe, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# Print tokens as they're generated\n",
    "for text in streamer:\n",
    "    print(text, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355bb78",
   "metadata": {},
   "source": [
    "## Running the Generation\n",
    "\n",
    "When you run the code above, you'll see the model generating text token by token in real-time.\n",
    "The streaming process allows for immediate feedback and a more interactive experience.\n",
    "\n",
    "### What's Happening Behind the Scenes?\n",
    "\n",
    "1. The main thread sets up the generation parameters and creates a streamer\n",
    "2. A worker thread is spawned to handle the actual text generation\n",
    "3. As tokens are generated, they're sent to the streamer's queue\n",
    "4. The main thread reads from the queue and prints tokens in real-time\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "- Streaming has minimal overhead compared to regular generation\n",
    "- Memory usage remains constant regardless of output length\n",
    "- CPU usage is distributed between threads\n",
    "- GPU utilization remains high during generation\n",
    "\n",
    "---\n",
    "\n",
    "## Try Different Queries\n",
    "\n",
    "Here are some example queries to try:\n",
    "\n",
    "### Knowledge-Based\n",
    "```python\n",
    "queries = [\n",
    "    \"Explain quantum entanglement\",\n",
    "    \"What is the theory of relativity?\",\n",
    "    \"How do neural networks work?\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Creative\n",
    "```python\n",
    "queries = [\n",
    "    \"Write a short story about time travel\",\n",
    "    \"Compose a haiku about autumn\",\n",
    "    \"Create a recipe for a unique sandwich\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Technical\n",
    "```python\n",
    "queries = [\n",
    "    \"Explain how garbage collection works in Python\",\n",
    "    \"What are the SOLID principles in software design?\",\n",
    "    \"Compare different sorting algorithms\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Analytical\n",
    "```python\n",
    "queries = [\n",
    "    \"Analyze the impact of social media on society\",\n",
    "    \"Compare renewable energy sources\",\n",
    "    \"Discuss the future of artificial intelligence\"\n",
    "]\n",
    "```\n",
    "\n",
    "To try a different query, simply modify the `text_inputs` in the `generation_kwargs`:\n",
    "\n",
    "```python\n",
    "generation_kwargs['text_inputs'] = [\n",
    "    {'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
    "    {'role': 'user', 'content': 'Your new query here'}\n",
    "]\n",
    "```\n",
    "\n",
    "### Experiment with Parameters\n",
    "\n",
    "Try adjusting these parameters to see how they affect the output:\n",
    "\n",
    "| Parameter     | Range  | Effect                    |\n",
    "|:-------------|:------:|:--------------------------|\n",
    "| Temperature  | 0-1    | Higher = more random      |\n",
    "| Top-p        | 0-1    | Higher = more diverse     |\n",
    "| Max tokens   | 1-2048 | Controls response length  |\n",
    "\n",
    "Example:\n",
    "```python\n",
    "generation_kwargs.update({\n",
    "    'temperature': 0.9,  # More creative\n",
    "    'top_p': 0.99,      # More diverse\n",
    "    'max_new_tokens': 1024  # Shorter response\n",
    "})\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
