{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d618956",
   "metadata": {},
   "source": [
    "# üöÄ Local LLM Tutorial: Streaming Text Generation\n",
    "\n",
    "## üìù Abstract\n",
    "\n",
    "This tutorial demonstrates how to implement streaming text generation using local language models.\n",
    "We'll explore the Hugging Face Transformers library's pipeline abstraction, focusing on its\n",
    "components, streaming capabilities, and generation parameters. The tutorial covers both the\n",
    "practical implementation and the underlying mechanisms that make streaming generation possible.\n",
    "\n",
    "## üåü Introduction\n",
    "\n",
    "Language models have become increasingly powerful, but their practical deployment often requires\n",
    "efficient handling of text generation, especially in interactive applications. Streaming generation\n",
    "is crucial for creating responsive user experiences, as it allows for real-time display of\n",
    "generated text rather than waiting for the entire output.\n",
    "\n",
    "In this tutorial, we'll:\n",
    "\n",
    "1. Set up a local language model using the Hugging Face pipeline\n",
    "2. Implement streaming text generation\n",
    "3. Configure generation parameters for optimal output\n",
    "4. Understand the underlying components and their interactions\n",
    "\n",
    "We'll use a suitable language model available on the Hugging Face Hub. The concepts and\n",
    "techniques covered are applicable to most transformer-based language models.\n",
    "\n",
    "**Note**: Some models (such as Mistral models) on the Hugging Face Hub require accepting terms of use (gated models).\n",
    "To use these models, you'll need to:\n",
    "  - Log in to your Hugging Face account (or create one).\n",
    "  - Accept the terms on the model's page.\n",
    "  - Set your Hugging Face API token as the `HF_TOKEN` variable in a `.env` file in the project root.\n",
    "\n",
    "The tutorial assumes basic familiarity with Python and machine learning concepts, but we'll\n",
    "explain each component in detail to ensure understanding of both the implementation and the\n",
    "underlying mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "import torch\n",
    "import dotenv\n",
    "from transformers import pipeline\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "dotenv.load_dotenv('.env')\n",
    "\n",
    "# --- Choose a Model --- #\n",
    "model_id = 'deepcogito/cogito-v1-preview-llama-3B'\n",
    "#model_id = 'HuggingFaceTB/SmolLM2-1.7B-Instruct'\n",
    "#model_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "#model_id = 'deepcogito/cogito-v1-preview-llama-3B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf89e6",
   "metadata": {},
   "source": [
    "## üîç Understanding the Pipeline\n",
    "\n",
    "The Hugging Face pipeline is a high-level abstraction that combines several components:\n",
    "\n",
    "### üß† Model\n",
    "- Core language model (in this case, a causal language model)\n",
    "- Uses transformer architecture\n",
    "- Contains the model weights and architecture definition\n",
    "- Handles the forward pass through the network\n",
    "\n",
    "### üî§ Tokenizer\n",
    "- Converts between text and token IDs\n",
    "- Splits text into tokens (subwords or words)\n",
    "- Handles special tokens (BOS, EOS, padding)\n",
    "- Manages vocabulary and token mappings\n",
    "\n",
    "### ‚öôÔ∏è Preprocessor\n",
    "- Prepares inputs for the model\n",
    "- Applies chat templates\n",
    "- Handles padding and truncation\n",
    "- Manages attention masks\n",
    "\n",
    "### üõ†Ô∏è Postprocessor\n",
    "- Processes model outputs\n",
    "- Decodes token IDs back to text\n",
    "- Handles special token removal\n",
    "- Manages output formatting\n",
    "\n",
    "The pipeline orchestrates these components in sequence:\n",
    "```\n",
    "Input Text ‚Üí Tokenizer ‚Üí Preprocessor ‚Üí Model ‚Üí Postprocessor ‚Üí Output Text\n",
    "```\n",
    "\n",
    "For text generation specifically, the pipeline also handles:\n",
    "- üìä Generation strategies (greedy, sampling, beam search)\n",
    "- üìè Length control (min/max length, early stopping)\n",
    "- üå°Ô∏è Temperature and top-k/p sampling\n",
    "- üîÑ Repetition penalties\n",
    "\n",
    "When we create a pipeline with `device_map=\"auto\"`, it also:\n",
    "1. Loads the model weights\n",
    "2. Determines available hardware (CPU/GPU)\n",
    "3. Splits the model across devices if needed\n",
    "4. Optimizes memory usage with the specified dtype (bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f2d5c",
   "metadata": {},
   "source": [
    "## üì• Download the Model (Optional but Recommended)\n",
    "\n",
    "While the Transformers library will automatically download model files when initializing the pipeline,\n",
    "using the HuggingFace CLI tool offers several advantages:\n",
    "\n",
    "- **Faster downloads**: Uses optimized downloading with multiple connections\n",
    "- **Better error handling**: Clearer feedback and retry mechanisms\n",
    "- **Explicit caching**: More control over where and how models are stored\n",
    "- **Progress tracking**: Visual feedback on download progress\n",
    "- **Offline usage**: Download once, use anywhere without internet connection\n",
    "\n",
    "The CLI command below is optional but recommended for a better experience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7b6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download --repo-type model {model_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc083627",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Model Loading\n",
    "\n",
    "First, we load the model using the Hugging Face pipeline. The pipeline provides a high-level\n",
    "interface for text generation tasks. Key parameters:\n",
    "\n",
    "- `model`: The model identifier from Hugging Face Hub (selected above)\n",
    "- `model_kwargs`: Additional arguments passed to the model initialization\n",
    "  - `torch_dtype`: Specifies the data type for model weights (bfloat16 for better memory efficiency)\n",
    "- `device_map`: Automatically handles device placement (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70408d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_id,\n",
    "    model_kwargs={'torch_dtype': torch.bfloat16},\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ef49e",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Setting Up Streaming\n",
    "\n",
    "We use `TextIteratorStreamer` to enable token-by-token streaming of the model's output.\n",
    "Key parameters:\n",
    "\n",
    "- `tokenizer`: The model's tokenizer for decoding tokens\n",
    "- `skip_prompt`: Skip the input prompt in the output\n",
    "- `skip_special_tokens`: Skip special tokens like <|endoftext|> in the output\n",
    "\n",
    "### How the Streamer Works\n",
    "1. Creates a queue to receive tokens\n",
    "2. Registering a callback with the model\n",
    "3. Decoding tokens as they arrive\n",
    "4. Yielding decoded text to the consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5999bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextIteratorStreamer(\n",
    "    pipe.tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b4f25",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Preparing the Chat\n",
    "\n",
    "We format our conversation using the chat template format. Each message has:\n",
    "- `role`: The speaker's role (system, user, assistant)\n",
    "- `content`: The message content\n",
    "\n",
    "The system message sets the model's behavior, while the user message contains the query.\n",
    "\n",
    "### Chat Template Processing\n",
    "1. Adds special tokens for message boundaries\n",
    "2. Formats roles and content\n",
    "3. Adds generation prompts\n",
    "4. Ensures proper tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a pirate chatbot who always responds in pirate speak!'},\n",
    "    {'role': 'user', 'content': 'What are black holes?'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bae88c",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Generation Parameters\n",
    "\n",
    "We configure the text generation with several important parameters:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `text_inputs` | The formatted chat messages |\n",
    "| `streamer` | Our streaming handler |\n",
    "| `max_new_tokens` | Maximum number of tokens to generate |\n",
    "| `do_sample` | Enable sampling for more diverse outputs |\n",
    "| `temperature` | Controls randomness (higher = more creative, lower = more focused) |\n",
    "| `top_p` | Nucleus sampling parameter (controls diversity of word choices) |\n",
    "\n",
    "### Generation Process\n",
    "1. Tokenizes the input\n",
    "2. Creates attention masks\n",
    "3. Runs the model forward pass\n",
    "4. Samples from the output distribution\n",
    "5. Updates the input sequence\n",
    "6. Repeats until max tokens or stop condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    'text_inputs': messages,\n",
    "    'streamer': streamer,\n",
    "    'max_new_tokens': 2048,\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.95,    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfed3f",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Running the Generation\n",
    "\n",
    "We run the generation in a separate thread to enable streaming. This allows us to:\n",
    "1. Start the generation process\n",
    "2. Print tokens as they're generated\n",
    "3. Maintain a responsive interface\n",
    "\n",
    "The `Thread` class from Python's threading module handles this asynchronous execution.\n",
    "\n",
    "### Threading Process\n",
    "1. Main thread: Sets up the streamer and starts generation\n",
    "2. Worker thread: Runs the model generation\n",
    "3. Streamer: Bridges between threads, handling token queue\n",
    "4. Main thread: Consumes and prints tokens as they arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8960cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = Thread(target=pipe, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# Print the generated text as it comes in\n",
    "for text in streamer:\n",
    "    print(text, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bbeadd",
   "metadata": {},
   "source": [
    "## üß™ Try Different Queries\n",
    "\n",
    "Feel free to modify the `messages` variable to explore different prompts and system messages.\n",
    "Here are some ideas:\n",
    "\n",
    "```python\n",
    "# Example 1: Different system prompt and query\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant who explains complex topics simply.'},\n",
    "    {'role': 'user', 'content': 'Explain quantum entanglement like I\\'m five.'},\n",
    "]\n",
    "\n",
    "# Example 2: Multi-turn conversation\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a friendly chatbot.'},\n",
    "    {'role': 'user', 'content': 'What\\'s the weather like today?'},\n",
    "    # Imagine the model responds, then you add the next turn:\n",
    "    # {'role': 'assistant', 'content': 'It\\'s sunny and warm!'},\n",
    "    # {'role': 'user', 'content': 'Great! Any recommendations for outdoor activities?'},\n",
    "]\n",
    "\n",
    "# Example 3: Creative writing prompt\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a famous fantasy author.'},\n",
    "    {'role': 'user', 'content': 'Write the opening paragraph of a story about a dragon who loves baking.'},\n",
    "]\n",
    "```\n",
    "Remember to re-run the cells after changing the `messages` to see the new output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44fa93",
   "metadata": {},
   "source": [
    "## üîÑ Understanding the Process\n",
    "\n",
    "### Step-by-Step Overview\n",
    "\n",
    "1. **Model Loading**: The pipeline loads the model and tokenizer, handling all the necessary\n",
    "   initialization and device placement.\n",
    "\n",
    "2. **Streaming Setup**: The `TextIteratorStreamer` creates a queue that receives tokens as they're\n",
    "   generated, allowing us to process them in real-time.\n",
    "\n",
    "3. **Message Formatting**: The chat template converts our messages into the format the model expects,\n",
    "   including special tokens and role markers.\n",
    "\n",
    "4. **Generation**: The model generates text token by token, with sampling parameters controlling\n",
    "   the creativity and diversity of the output.\n",
    "\n",
    "5. **Output**: Tokens are streamed through the `TextIteratorStreamer`, decoded, and printed in\n",
    "   real-time, creating a responsive chat experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafff9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "auto:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
